{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "y pred = [0. 0. 0. 0.]\n",
      "loss = 30.0\n",
      "gradient = -120.0\n",
      "weight = 1.2\n",
      "******************************\n",
      "y pred = [1.2       2.4       3.6000001 4.8      ]\n",
      "loss = 4.799999237060547\n",
      "gradient = -48.0\n",
      "weight = 1.68\n",
      "******************************\n",
      "y pred = [1.68 3.36 5.04 6.72]\n",
      "loss = 0.7680001854896545\n",
      "gradient = -19.200002670288086\n",
      "weight = 1.8720000267028807\n",
      "******************************\n",
      "y pred = [1.872 3.744 5.616 7.488]\n",
      "loss = 0.1228799968957901\n",
      "gradient = -7.679999828338623\n",
      "weight = 1.948800024986267\n",
      "******************************\n",
      "y pred = [1.9488 3.8976 5.8464 7.7952]\n",
      "loss = 0.019660834223031998\n",
      "gradient = -3.072002649307251\n",
      "weight = 1.9795200514793394\n",
      "******************************\n",
      "Prediction after the training: f(5) = 9.898\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # do everything with numpy\n",
    "# f = w * x such as f = 2 * x\n",
    "X = np.array([1,2,3,4], dtype = np.float32)\n",
    "y = np.array([2,4,6,8], dtype = np.float32)   #2 * x\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "#model prediction\n",
    "def forward(x):\n",
    "    return w * x  #forward pass\n",
    "\n",
    "#loss\n",
    "def loss(y, y_pred):\n",
    "    return ((y - y_pred)**2).mean()\n",
    "\n",
    "#gradient , MSE = 1/N * (w*x -y)**2\n",
    "# dJ/dw = 1/N 2x (w*x - y)\n",
    "def gradient(x,y, y_pred):\n",
    "    return np.dot(2*x, y_pred - y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "#training\n",
    "learning_rate = 0.01\n",
    "n_iters = 5\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    print(f'y pred = {y_pred}')\n",
    "\n",
    "    #loss\n",
    "    l = loss(y, y_pred)\n",
    "    print(f'loss = {l}')\n",
    "\n",
    "    #gradients\n",
    "    dw = gradient(X, y, y_pred)\n",
    "    print(f'gradient = {dw}')\n",
    "    #update weights\n",
    "    w -= learning_rate * dw\n",
    "    print(f'weight = {w}')\n",
    "    print(\"******************************\")\n",
    "    if epoch % 5 == 0:\n",
    "        pass\n",
    "        #print(f'epoch {epoch + 1}: w  = {w:.3f}, loss = {l:.8f}')\n",
    "    \n",
    "print(f\"Prediction after the training: f(5) = {forward(5):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w  = 0.300, loss = 30.00000000\n",
      "epoch 11: w  = 1.665, loss = 1.16278565\n",
      "epoch 21: w  = 1.934, loss = 0.04506890\n",
      "epoch 31: w  = 1.987, loss = 0.00174685\n",
      "epoch 41: w  = 1.997, loss = 0.00006770\n",
      "epoch 51: w  = 1.999, loss = 0.00000262\n",
      "epoch 61: w  = 2.000, loss = 0.00000010\n",
      "epoch 71: w  = 2.000, loss = 0.00000000\n",
      "epoch 81: w  = 2.000, loss = 0.00000000\n",
      "epoch 91: w  = 2.000, loss = 0.00000000\n",
      "Prediction after the training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "import torch  # do everything with torch\n",
    "# f = w * x such as f = 2 * x\n",
    "X = torch.tensor([1,2,3,4], dtype = torch.float32)\n",
    "y = torch.tensor([2,4,6,8], dtype = torch.float32)   #2 * x\n",
    "\n",
    "w = torch.tensor(0.0, dtype = torch.float32, requires_grad=True)\n",
    "\n",
    "#model prediction\n",
    "def forward(x):\n",
    "    return w * x  #forward pass\n",
    "\n",
    "#loss\n",
    "def loss(y, y_pred):\n",
    "    return ((y - y_pred)**2).mean()\n",
    "\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "#training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    #loss\n",
    "    l = loss(y, y_pred)\n",
    "\n",
    "    #gradients = backward pass\n",
    "    l.backward()  # dl/dw\n",
    "\n",
    "    #update weights\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "\n",
    "    # zero gradients : you need to empty the gradients before the next iter.\n",
    "    w.grad.zero_()  #underscore at the end means it will modify, make changes\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch + 1}: w  = {w:.3f}, loss = {l:.8f}')\n",
    "    \n",
    "print(f\"Prediction after the training: f(5) = {forward(5):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Design model (input, output size, forward pass)\n",
    "# 2) Construct loss and optimizer\n",
    "# 3) Training loop\n",
    "#   - forward pass: compute prediction\n",
    "#   - backward pass: gradients\n",
    "#   - update weights\n",
    "\n",
    "# thats the whole pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n",
      "4 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/hamzagorgulu/Desktop/software/Pytorch/deep_learning_with_pytorch.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamzagorgulu/Desktop/software/Pytorch/deep_learning_with_pytorch.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamzagorgulu/Desktop/software/Pytorch/deep_learning_with_pytorch.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# model = nn.Linear(input_size, output_size)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hamzagorgulu/Desktop/software/Pytorch/deep_learning_with_pytorch.ipynb#X10sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m model \u001b[39m=\u001b[39m My_Model(input_size, output_size)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamzagorgulu/Desktop/software/Pytorch/deep_learning_with_pytorch.ipynb#X10sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPrediction before training: f(5) = \u001b[39m\u001b[39m{\u001b[39;00mmodel(X_test)\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamzagorgulu/Desktop/software/Pytorch/deep_learning_with_pytorch.ipynb#X10sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# loss which is provided by pytorch\u001b[39;00m\n",
      "\u001b[1;32m/Users/hamzagorgulu/Desktop/software/Pytorch/deep_learning_with_pytorch.ipynb Cell 6\u001b[0m in \u001b[0;36mMy_Model.__init__\u001b[0;34m(self, input_dim, output_dim)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamzagorgulu/Desktop/software/Pytorch/deep_learning_with_pytorch.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m,input_dim,output_dim):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hamzagorgulu/Desktop/software/Pytorch/deep_learning_with_pytorch.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39msuper\u001b[39;49m(LinearRegression, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamzagorgulu/Desktop/software/Pytorch/deep_learning_with_pytorch.ipynb#X10sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m#define layers\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamzagorgulu/Desktop/software/Pytorch/deep_learning_with_pytorch.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(input_dim, output_dim)\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import torch.nn as nn\n",
    " # initalization is random, you get different result everytime you run\n",
    "X = torch.tensor([[1],[2],[3],[4]], dtype = torch.float32)\n",
    "y = torch.tensor([[2],[4],[6],[8]], dtype = torch.float32)   #2 * x\n",
    "# no need to define weights, pytorch module knows the parameters\n",
    "X_test = torch.tensor([5], dtype = torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(X.shape)\n",
    "print(n_samples,n_features)\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "class My_Model(nn.Module): #linear regression, dummy example\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        #define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "\n",
    "\n",
    "# model = nn.Linear(input_size, output_size)\n",
    "model = My_Model(input_size, output_size)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "# loss which is provided by pytorch\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = model(X)\n",
    "\n",
    "    #loss\n",
    "    l = loss(y, y_pred)\n",
    "\n",
    "    #gradients = backward pass\n",
    "    l.backward()  # dl/dw\n",
    "\n",
    "    #no need to update weights manually, use optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero gradients : you need to empty the gradients before the next iter.\n",
    "    optimizer.zero_grad()  \n",
    "    if epoch % 10 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f'epoch {epoch + 1}: w  = {w[0][0].item():.3f}, loss = {l:.8f}') \n",
    "print(f\"Prediction after the training: f(5) = {model(X_test).item():.3f}\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
